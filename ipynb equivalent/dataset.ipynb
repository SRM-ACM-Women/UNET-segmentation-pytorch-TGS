{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataset.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMfsHBd01tKlhlQacIe5jxl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rayruchira/UNET-segmentation-pytorch-TGS/blob/main/ipynb%20equivalent/dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaD5eJvz-iNh"
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpq0GgL5WNZo"
      },
      "source": [
        "import torch\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.transforms import Compose, ToTensor, Resize\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AScrK3sC3__P"
      },
      "source": [
        "# for splitting it externally\n",
        "\n",
        "def train_val_dataset(dataset, val_split=0.25):\n",
        "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
        "    datasets = {}\n",
        "    datasets['train'] = Subset(dataset, train_idx)\n",
        "    datasets['val'] = Subset(dataset, val_idx)\n",
        "    return datasets\n",
        "\n",
        "dataset = ImageFolder('C:\\Datasets\\lcms-dataset', transform=Compose([Resize((224,224)),ToTensor()]))\n",
        "print(len(dataset))\n",
        "datasets = train_val_dataset(dataset)\n",
        "print(len(datasets['train']))\n",
        "print(len(datasets['val']))\n",
        "# The original dataset is available in the Subset class\n",
        "print(datasets['train'].dataset)\n",
        "\n",
        "dataloaders = {x:DataLoader(datasets[x],32, shuffle=True, num_workers=4) for x in ['train','val']}\n",
        "x,y = next(iter(dataloaders['train']))\n",
        "print(x.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5FVvvdZ_KTK"
      },
      "source": [
        "class TGSdataset(Dataset):\n",
        "  def __init__(self, img, mask, transform=None):\n",
        "    #basic logging of directories img = og image directory, mask= mask of images directory\n",
        "    self.imgP=img\n",
        "    self.maskP=mask\n",
        "    self.transform=transform\n",
        "\n",
        "    #total no. of images\n",
        "    self.images=os.listdir(img)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "  \n",
        "  def __getitem__(self,index):\n",
        "    imgpath=os.path.join(self.imgP, self.images[index])\n",
        "    maskpath=os.path.join(self.maskP, self.images[index]) #name of mask file same as normal file or would have to edit the images[index]part\n",
        "    image=np.arry(Image.open(imgpath).convert(\"L\"), dtype=np.float32) #might not be needed, could already be in grayscale( could also be RGB)\n",
        "    mask=np.arry(Image.open(imgpath).convert(\"L\"), dtype=np.float32) #might not be needed, could already be in grayscale\n",
        "    mask[mask== 255.0] =1.0 #change 255 entries to 1, as we will use sigmid for last activation, 1 is white\n",
        "\n",
        "    if self.transform is not None:\n",
        "      augmentations= self.transform(image=image, mask=mask)\n",
        "      image =augmentations[\"image\"]\n",
        "      mask=augmentations[\"mask\"]\n",
        "\n",
        "    return image,mask\n",
        "    \n",
        "  def train_val_dataset(dataset, val_split=0.25):\n",
        "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
        "    datasets = {}\n",
        "    datasets['train'] = Subset(dataset, train_idx)\n",
        "    datasets['val'] = Subset(dataset, val_idx)\n",
        "    return datasets\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gydeATC5WFwM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}